{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "## Import NLTK packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "## Import Spacy\n",
    "import spacy\n",
    "\n",
    "## Import Gensim packages\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "## Notebook display settings\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data//lyric_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3b7a4bd12fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlyrics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data//lyric_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data//lyric_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "lyrics_df = pd.read_csv('data//lyric_data.csv', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_lyrics = lyrics_df[lyrics_df.artist == 'The Beatles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_lyrics.drop(columns = ['link'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_lyrics.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_lyrics.rename(columns = {'text' : 'lyrics'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_lyrics.to_csv('data//beatles_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles_lyrics = pd.read_csv('data//beatles_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of lyrics\n",
    "\n",
    "stripped_lyrics = beatles_lyrics.lyrics.values.tolist()\n",
    "stripped_lyrics = [x.lower() for x in stripped_lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning up the text - removing new lines, extra spaces, random characters\n",
    "\n",
    "stripped_lyrics = [re.sub('\\n', '', sent) for sent in stripped_lyrics]\n",
    "stripped_lyrics = [re.sub('\\s{2,}', ' ', sent) for sent in stripped_lyrics]\n",
    "stripped_lyrics = [re.sub(\"\\\\\\'\", '', sent) for sent in stripped_lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stripped_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing a stop word list and including a few extra words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['hey', 'nah', '[chorus]', 'la', 'na', 'ta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing the lyrics in each song using RegexpTokenizer and removing stop words\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "token_list = []\n",
    "\n",
    "for i in range(len(stripped_lyrics)):   \n",
    "    tokenized_list = []\n",
    "    #tokenized_list = tokenizer.tokenize(stripped_lyrics[i])\n",
    "    tokenized_list = word_tokenize(stripped_lyrics[i])\n",
    "    tokenized_list = [i for i in tokenized_list if i not in stop_words]\n",
    "    #tokenized_list = [pos_tag(word_tokenize(tokens), tagset='universal') for tokens in tokenized_list]\n",
    "    #tokenized_list = [porter.stem(tokens) for tokens in tokenized_list]\n",
    "    token_list.append(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'VERB', 'ADJ', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = lemmatization(token_list, allowed_postags=['NOUN', 'VERB', 'ADJ', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(token_list, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[token_list], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence labeled as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(token_list)\n",
    "data_words_trigrams = make_trigrams(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 2), (6, 3), (7, 1), (8, 3), (9, 1), (10, 3), (11, 3), (12, 6), (13, 1), (14, 3), (15, 1), (16, 1), (17, 15), (18, 3), (19, 2), (20, 1), (21, 1), (22, 3), (23, 2), (24, 3), (25, 2), (26, 3), (27, 2), (28, 1), (29, 1), (30, 8), (31, 3), (32, 1), (33, 1), (34, 1), (35, 7), (36, 2), (37, 3), (38, 2), (39, 3), (40, 3), (41, 4), (42, 5), (43, 3), (44, 1), (45, 5), (46, 1), (47, 8), (48, 1), (49, 1), (50, 3), (51, 3), (52, 1), (53, 8), (54, 3)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_trigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_trigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=13,\n",
    "                                           random_state=42,\n",
    "                                           update_every=10,\n",
    "                                           chunksize=50,\n",
    "                                           passes=4,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.067*\"go\" + 0.057*\"not\" + 0.052*\"do\" + 0.024*\"long\" + 0.020*\"be\" + '\n",
      "  '0.017*\"know\" + 0.014*\"never\" + 0.014*\"love\" + 0.014*\"see\" + 0.013*\"johnny\"'),\n",
      " (1,\n",
      "  '0.049*\"not\" + 0.029*\"be\" + 0.027*\"do\" + 0.018*\"make\" + 0.016*\"say\" + '\n",
      "  '0.016*\"know\" + 0.016*\"buy\" + 0.015*\"back\" + 0.015*\"help\" + 0.014*\"get\"'),\n",
      " (2,\n",
      "  '0.056*\"be\" + 0.034*\"know\" + 0.032*\"not\" + 0.020*\"love\" + 0.019*\"little\" + '\n",
      "  '0.017*\"come\" + 0.017*\"do\" + 0.014*\"say\" + 0.013*\"will\" + 0.013*\"would\"'),\n",
      " (3,\n",
      "  '0.039*\"s\" + 0.032*\"get\" + 0.024*\"not\" + 0.020*\"be\" + 0.017*\"want\" + '\n",
      "  '0.017*\"good\" + 0.016*\"love\" + 0.016*\"have\" + 0.014*\"girl\" + 0.014*\"go\"'),\n",
      " (4,\n",
      "  '0.054*\"be\" + 0.044*\"baby\" + 0.036*\"get\" + 0.032*\"not\" + 0.029*\"love\" + '\n",
      "  '0.025*\"ill\" + 0.018*\"want\" + 0.013*\"long\" + 0.012*\"know\" + 0.012*\"can\"'),\n",
      " (5,\n",
      "  '0.040*\"together\" + 0.028*\"want\" + 0.024*\"cry\" + 0.024*\"man\" + 0.022*\"baby\" '\n",
      "  '+ 0.022*\"be\" + 0.020*\"love\" + 0.015*\"back\" + 0.015*\"bring\" + 0.014*\"gon\"'),\n",
      " (6,\n",
      "  '0.039*\"love\" + 0.039*\"hold\" + 0.036*\"day\" + 0.031*\"hand\" + 0.019*\"get\" + '\n",
      "  '0.019*\"good\" + 0.017*\"go\" + 0.017*\"be\" + 0.017*\"want\" + 0.016*\"sunshine\"'),\n",
      " (7,\n",
      "  '0.089*\"love\" + 0.050*\"say\" + 0.031*\"need\" + 0.031*\"know\" + 0.027*\"time\" + '\n",
      "  '0.025*\"want\" + 0.025*\"not\" + 0.024*\"be\" + 0.019*\"do\" + 0.017*\"s\"'),\n",
      " (8,\n",
      "  '0.051*\"back\" + 0.047*\"get\" + 0.023*\"leave\" + 0.021*\"not\" + '\n",
      "  '0.018*\"dear_prudence\" + 0.017*\"birthday\" + 0.016*\"better\" + 0.014*\"julia\" + '\n",
      "  '0.014*\"alone\" + 0.013*\"belong\"'),\n",
      " (9,\n",
      "  '0.031*\"be\" + 0.024*\"cry\" + 0.019*\"roll\" + 0.019*\"make\" + 0.018*\"s\" + '\n",
      "  '0.014*\"roll_mystery_tour\" + 0.012*\"take\" + 0.012*\"see\" + 0.012*\"get\" + '\n",
      "  '0.011*\"come\"'),\n",
      " (10,\n",
      "  '0.067*\"not\" + 0.056*\"do\" + 0.036*\"let\" + 0.021*\"be\" + 0.020*\"get\" + '\n",
      "  '0.020*\"know\" + 0.015*\"honey\" + 0.015*\"come\" + 0.011*\"would\" + 0.011*\"hand\"'),\n",
      " (11,\n",
      "  '0.116*\"come\" + 0.054*\"sun\" + 0.039*\"get\" + 0.030*\"together\" + 0.028*\"easy\" '\n",
      "  '+ 0.017*\"say\" + 0.015*\"right\" + 0.015*\"take\" + 0.015*\"joy\" + 0.014*\"let\"'),\n",
      " (12,\n",
      "  '0.041*\"ill\" + 0.027*\"anything\" + 0.024*\"baby\" + 0.022*\"love\" + '\n",
      "  '0.020*\"drive_car\" + 0.019*\"want\" + 0.016*\"spite\" + 0.014*\"life\" + '\n",
      "  '0.014*\"will\" + 0.013*\"true\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -5.901105077234152\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF on lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_1 = CountVectorizer()\n",
    "matrix = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False).fit_transform(token_list)\n",
    "matrix.toarray()\n",
    "#matrix = vectorizer.fit_transform(token_list)\n",
    "#print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 676)\t3\n",
      "  (0, 1400)\t5\n",
      "  (0, 677)\t2\n",
      "  (0, 96)\t3\n",
      "  (0, 517)\t3\n",
      "  (0, 286)\t3\n",
      "  (0, 246)\t3\n",
      "  (0, 1353)\t3\n",
      "  (0, 340)\t3\n",
      "  (0, 1528)\t3\n",
      "  (0, 1701)\t3\n",
      "  (0, 1305)\t3\n",
      "  (0, 1541)\t3\n",
      "  (0, 1253)\t3\n",
      "  (0, 596)\t3\n",
      "  (0, 93)\t3\n",
      "  (0, 807)\t3\n",
      "  (0, 397)\t6\n",
      "  (0, 888)\t3\n",
      "  (0, 1651)\t8\n",
      "  (0, 1330)\t3\n",
      "  (0, 342)\t3\n",
      "  (0, 1091)\t3\n",
      "  (0, 942)\t2\n",
      "  (0, 597)\t2\n",
      "  :\t:\n",
      "  (177, 1289)\t1\n",
      "  (177, 1512)\t1\n",
      "  (177, 1704)\t1\n",
      "  (177, 1303)\t1\n",
      "  (177, 1518)\t1\n",
      "  (177, 718)\t3\n",
      "  (177, 872)\t1\n",
      "  (177, 1273)\t2\n",
      "  (177, 1671)\t4\n",
      "  (177, 1633)\t2\n",
      "  (177, 886)\t8\n",
      "  (177, 1242)\t1\n",
      "  (177, 907)\t2\n",
      "  (177, 236)\t1\n",
      "  (177, 1400)\t2\n",
      "  (177, 286)\t4\n",
      "  (177, 1701)\t1\n",
      "  (177, 93)\t16\n",
      "  (177, 807)\t3\n",
      "  (177, 1330)\t1\n",
      "  (177, 1044)\t5\n",
      "  (177, 207)\t1\n",
      "  (177, 579)\t2\n",
      "  (177, 520)\t1\n",
      "  (177, 1435)\t1\n"
     ]
    }
   ],
   "source": [
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False).fit_transform(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 676)\t3\n",
      "  (0, 1400)\t5\n",
      "  (0, 677)\t2\n",
      "  (0, 96)\t3\n",
      "  (0, 517)\t3\n",
      "  (0, 286)\t3\n",
      "  (0, 246)\t3\n",
      "  (0, 1353)\t3\n",
      "  (0, 340)\t3\n",
      "  (0, 1528)\t3\n",
      "  (0, 1701)\t3\n",
      "  (0, 1305)\t3\n",
      "  (0, 1541)\t3\n",
      "  (0, 1253)\t3\n",
      "  (0, 596)\t3\n",
      "  (0, 93)\t3\n",
      "  (0, 807)\t3\n",
      "  (0, 397)\t6\n",
      "  (0, 888)\t3\n",
      "  (0, 1651)\t8\n",
      "  (0, 1330)\t3\n",
      "  (0, 342)\t3\n",
      "  (0, 1091)\t3\n",
      "  (0, 942)\t2\n",
      "  (0, 597)\t2\n",
      "  :\t:\n",
      "  (177, 1289)\t1\n",
      "  (177, 1512)\t1\n",
      "  (177, 1704)\t1\n",
      "  (177, 1303)\t1\n",
      "  (177, 1518)\t1\n",
      "  (177, 718)\t3\n",
      "  (177, 872)\t1\n",
      "  (177, 1273)\t2\n",
      "  (177, 1671)\t4\n",
      "  (177, 1633)\t2\n",
      "  (177, 886)\t8\n",
      "  (177, 1242)\t1\n",
      "  (177, 907)\t2\n",
      "  (177, 236)\t1\n",
      "  (177, 1400)\t2\n",
      "  (177, 286)\t4\n",
      "  (177, 1701)\t1\n",
      "  (177, 93)\t16\n",
      "  (177, 807)\t3\n",
      "  (177, 1330)\t1\n",
      "  (177, 1044)\t5\n",
      "  (177, 207)\t1\n",
      "  (177, 579)\t2\n",
      "  (177, 520)\t1\n",
      "  (177, 1435)\t1\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfVectorizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ac93042bd084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tfidf._validate_vocabulary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TfidfVectorizer' object is not iterable"
     ]
    }
   ],
   "source": [
    "#tfidf._validate_vocabulary()\n",
    "\n",
    "for i, feature in enumerate(vectorizer):\n",
    "    print(i, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
